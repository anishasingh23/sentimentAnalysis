{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anishasingh23/sentimentAnalysis/blob/main/sentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit pandas numpy matplotlib seaborn requests nltk plotly wordcloud"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VhUsW1XWlOcK",
        "outputId": "6180628a-9e46-4c19-eb1b-215fb406ef0a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.45.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.38.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sentiment_app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import re\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud\n",
        "import json\n",
        "\n",
        "# Initial setup\n",
        "st.set_page_config(\n",
        "    page_title=\"Sentiment Analysis Dashboard\",\n",
        "    page_icon=\"ðŸ˜Š\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# Download NLTK resources\n",
        "@st.cache_resource\n",
        "def download_nltk_resources():\n",
        "    try:\n",
        "        nltk.data.find('vader_lexicon')\n",
        "    except LookupError:\n",
        "        nltk.download('vader_lexicon')\n",
        "    try:\n",
        "        nltk.data.find('punkt')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt')\n",
        "\n",
        "download_nltk_resources()\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "@st.cache_resource\n",
        "def get_analyzer():\n",
        "    return SentimentIntensityAnalyzer()\n",
        "\n",
        "analyzer = get_analyzer()\n",
        "\n",
        "# Initialize session state variables if they don't exist\n",
        "if 'analyzed_data' not in st.session_state:\n",
        "    st.session_state.analyzed_data = pd.DataFrame(columns=['text', 'pos_score', 'neu_score', 'neg_score', 'compound', 'sentiment', 'confidence', 'timestamp'])\n",
        "if 'api_results' not in st.session_state:\n",
        "    st.session_state.api_results = []\n",
        "if 'api_type' not in st.session_state:\n",
        "    st.session_state.api_type = \"news\"\n",
        "if 'show_history' not in st.session_state:\n",
        "    st.session_state.show_history = False\n",
        "\n",
        "# Custom sentiment analysis function using VADER and custom logic\n",
        "def analyze_sentiment(text):\n",
        "    if not text:\n",
        "        return None\n",
        "\n",
        "    # Get VADER scores\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "\n",
        "    # Custom sentiment categorization with thresholds\n",
        "    compound = scores['compound']\n",
        "\n",
        "    # Define custom sentiment categories with confidence\n",
        "    if compound >= 0.05:\n",
        "        sentiment = \"Positive\"\n",
        "        # Higher absolute scores indicate higher confidence\n",
        "        confidence = min(abs(compound) * 2, 1.0) if compound > 0 else 0\n",
        "    elif compound <= -0.05:\n",
        "        sentiment = \"Negative\"\n",
        "        confidence = min(abs(compound) * 2, 1.0) if compound < 0 else 0\n",
        "    else:\n",
        "        sentiment = \"Neutral\"\n",
        "        # For neutral, confidence is highest when score is close to 0\n",
        "        confidence = 1 - min(abs(compound) * 5, 1.0)\n",
        "\n",
        "    return {\n",
        "        'pos_score': scores['pos'],\n",
        "        'neu_score': scores['neu'],\n",
        "        'neg_score': scores['neg'],\n",
        "        'compound': compound,\n",
        "        'sentiment': sentiment,\n",
        "        'confidence': confidence,\n",
        "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "\n",
        "# Function to fetch news from News API\n",
        "def fetch_news(query, api_key, days=7, language=\"en\"):\n",
        "    base_url = \"https://newsapi.org/v2/everything\"\n",
        "\n",
        "    # Calculate the date range (last week)\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=days)\n",
        "\n",
        "    # Format dates for the API\n",
        "    from_date = start_date.strftime(\"%Y-%m-%d\")\n",
        "    to_date = end_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    params = {\n",
        "        'q': query,\n",
        "        'language': language,\n",
        "        'from': from_date,\n",
        "        'to': to_date,\n",
        "        'sortBy': 'publishedAt',\n",
        "        'apiKey': api_key\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            error_message = f\"Error: {response.status_code} - {response.reason}\"\n",
        "            try:\n",
        "                error_data = response.json()\n",
        "                if 'message' in error_data:\n",
        "                    error_message += f\". {error_data['message']}\"\n",
        "            except:\n",
        "                pass\n",
        "            st.error(error_message)\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error fetching news: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Function to add data to history\n",
        "def add_to_history(text, analysis_result):\n",
        "    new_data = pd.DataFrame({\n",
        "        'text': [text],\n",
        "        'pos_score': [analysis_result['pos_score']],\n",
        "        'neu_score': [analysis_result['neu_score']],\n",
        "        'neg_score': [analysis_result['neg_score']],\n",
        "        'compound': [analysis_result['compound']],\n",
        "        'sentiment': [analysis_result['sentiment']],\n",
        "        'confidence': [analysis_result['confidence']],\n",
        "        'timestamp': [analysis_result['timestamp']]\n",
        "    })\n",
        "\n",
        "    st.session_state.analyzed_data = pd.concat([st.session_state.analyzed_data, new_data], ignore_index=True)\n",
        "\n",
        "# Function to extract keywords from text\n",
        "def extract_keywords(text, min_length=4):\n",
        "    # Simple keyword extraction by counting word frequency\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    # Remove short words\n",
        "    words = [word for word in words if len(word) >= min_length]\n",
        "    # Remove common stop words (simplified version)\n",
        "    stop_words = {'the', 'and', 'are', 'this', 'that', 'with', 'from', 'have', 'has', 'been', 'were', 'was', 'will', 'would', 'should', 'could', 'they', 'their', 'what', 'when', 'where', 'which', 'these', 'those', 'there', 'here', 'some'}\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Count word frequencies\n",
        "    word_freq = {}\n",
        "    for word in words:\n",
        "        if word in word_freq:\n",
        "            word_freq[word] += 1\n",
        "        else:\n",
        "            word_freq[word] = 1\n",
        "\n",
        "    # Sort by frequency\n",
        "    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_words[:10]  # Return top 10 keywords\n",
        "\n",
        "# Dashboard UI\n",
        "st.title(\"ðŸ“Š Sentiment Analysis Dashboard\")\n",
        "st.markdown(\"Analyze the sentiment of text using VADER and custom logic\")\n",
        "\n",
        "# Sidebar\n",
        "st.sidebar.title(\"Settings\")\n",
        "analysis_mode = st.sidebar.radio(\"Select Input Mode\", [\"Manual Input\", \"News API\"])\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "if analysis_mode == \"Manual Input\":\n",
        "    # Text input area\n",
        "    text_input = st.text_area(\"Enter text to analyze\", height=150)\n",
        "\n",
        "    if st.button(\"Analyze Sentiment\"):\n",
        "        if text_input:\n",
        "            with st.spinner(\"Analyzing sentiment...\"):\n",
        "                # Perform sentiment analysis\n",
        "                analysis_result = analyze_sentiment(text_input)\n",
        "\n",
        "                if analysis_result:\n",
        "                    # Add to history\n",
        "                    add_to_history(text_input, analysis_result)\n",
        "\n",
        "                    # Display results\n",
        "                    st.markdown(\"### Sentiment Analysis Results\")\n",
        "\n",
        "                    # Create layout with columns\n",
        "                    col1, col2 = st.columns(2)\n",
        "\n",
        "                    with col1:\n",
        "                        # Display sentiment and confidence\n",
        "                        sentiment_color = {\n",
        "                            \"Positive\": \"green\",\n",
        "                            \"Neutral\": \"blue\",\n",
        "                            \"Negative\": \"red\"\n",
        "                        }\n",
        "\n",
        "                        # Sentiment badge\n",
        "                        st.markdown(f\"\"\"\n",
        "                        <div style=\"display: flex; align-items: center; margin-bottom: 15px;\">\n",
        "                            <div style=\"background-color: {sentiment_color[analysis_result['sentiment']]};\n",
        "                                 color: white; padding: 8px 16px; border-radius: 20px; font-weight: bold;\">\n",
        "                                {analysis_result['sentiment']}\n",
        "                            </div>\n",
        "                            <div style=\"margin-left: 15px; font-size: 16px;\">\n",
        "                                Confidence: {analysis_result['confidence']:.2f}\n",
        "                            </div>\n",
        "                        </div>\n",
        "                        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "                        # Score breakdown\n",
        "                        st.markdown(\"#### Score Breakdown\")\n",
        "                        score_df = pd.DataFrame({\n",
        "                            'Score Type': ['Positive', 'Neutral', 'Negative', 'Compound'],\n",
        "                            'Value': [\n",
        "                                analysis_result['pos_score'],\n",
        "                                analysis_result['neu_score'],\n",
        "                                analysis_result['neg_score'],\n",
        "                                analysis_result['compound']\n",
        "                            ]\n",
        "                        })\n",
        "\n",
        "                        # Plot the score breakdown\n",
        "                        fig = px.bar(\n",
        "                            score_df,\n",
        "                            x='Score Type',\n",
        "                            y='Value',\n",
        "                            color='Score Type',\n",
        "                            color_discrete_map={\n",
        "                                'Positive': 'green',\n",
        "                                'Neutral': 'blue',\n",
        "                                'Negative': 'red',\n",
        "                                'Compound': 'purple'\n",
        "                            }\n",
        "                        )\n",
        "                        fig.update_layout(height=300)\n",
        "                        st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "                    with col2:\n",
        "                        # Keywords extraction\n",
        "                        keywords = extract_keywords(text_input)\n",
        "\n",
        "                        if keywords:\n",
        "                            st.markdown(\"#### Key Terms\")\n",
        "                            # Create word cloud\n",
        "                            wordcloud_dict = {word: freq for word, freq in keywords}\n",
        "\n",
        "                            if wordcloud_dict:\n",
        "                                wordcloud = WordCloud(\n",
        "                                    width=400,\n",
        "                                    height=200,\n",
        "                                    background_color='white',\n",
        "                                    colormap='viridis',\n",
        "                                    max_words=50\n",
        "                                ).generate_from_frequencies(wordcloud_dict)\n",
        "\n",
        "                                plt.figure(figsize=(10, 5))\n",
        "                                plt.imshow(wordcloud, interpolation='bilinear')\n",
        "                                plt.axis('off')\n",
        "                                st.pyplot(plt)\n",
        "\n",
        "                            # Keywords table\n",
        "                            keyword_df = pd.DataFrame(keywords, columns=['Term', 'Frequency'])\n",
        "                            st.table(keyword_df)\n",
        "                        else:\n",
        "                            st.info(\"No significant keywords found in the text.\")\n",
        "                else:\n",
        "                    st.error(\"Could not analyze the text. Please try again.\")\n",
        "        else:\n",
        "            st.warning(\"Please enter some text to analyze.\")\n",
        "\n",
        "elif analysis_mode == \"News API\":\n",
        "    st.sidebar.markdown(\"### News API Settings\")\n",
        "\n",
        "    # News API configuration\n",
        "    query = st.sidebar.text_input(\"Search Query\", \"technology\")\n",
        "    days = st.sidebar.slider(\"Days to look back\", min_value=1, max_value=30, value=7)\n",
        "    api_key = st.sidebar.text_input(\"News API Key\", type=\"password\")\n",
        "\n",
        "    if st.sidebar.button(\"Fetch News\"):\n",
        "        if api_key and query:\n",
        "            with st.spinner(\"Fetching news...\"):\n",
        "                news_data = fetch_news(query, api_key, days)\n",
        "\n",
        "                if news_data and 'articles' in news_data:\n",
        "                    articles = news_data['articles']\n",
        "\n",
        "                    if articles:\n",
        "                        st.session_state.api_results = articles\n",
        "                        st.session_state.api_type = \"news\"\n",
        "                        st.success(f\"Found {len(articles)} articles about '{query}'\")\n",
        "                    else:\n",
        "                        st.warning(f\"No articles found for query: {query}\")\n",
        "                        st.session_state.api_results = []\n",
        "                else:\n",
        "                    st.error(\"Failed to fetch news. Please check your API key and try again.\")\n",
        "        else:\n",
        "            st.warning(\"Please enter a search query and API key.\")\n",
        "\n",
        "    # Display and analyze fetched articles\n",
        "    if st.session_state.api_results:\n",
        "        st.markdown(f\"### News Articles: {len(st.session_state.api_results)} Results\")\n",
        "\n",
        "        # Analyze all articles\n",
        "        if st.button(\"Analyze All Articles\"):\n",
        "            with st.spinner(\"Analyzing articles...\"):\n",
        "                progress_bar = st.progress(0)\n",
        "\n",
        "                for i, article in enumerate(st.session_state.api_results):\n",
        "                    title = article.get('title', '')\n",
        "                    description = article.get('description', '')\n",
        "                    content = article.get('content', '')\n",
        "\n",
        "                    # Combine text for analysis\n",
        "                    text = f\"{title}. {description} {content}\"\n",
        "\n",
        "                    # Analyze sentiment\n",
        "                    analysis_result = analyze_sentiment(text)\n",
        "\n",
        "                    if analysis_result:\n",
        "                        # Add article information to the result\n",
        "                        analysis_result['source'] = article.get('source', {}).get('name', 'Unknown')\n",
        "                        analysis_result['title'] = title\n",
        "                        analysis_result['url'] = article.get('url', '')\n",
        "                        analysis_result['publishedAt'] = article.get('publishedAt', '')\n",
        "\n",
        "                        # Add to history\n",
        "                        add_to_history(title, analysis_result)\n",
        "\n",
        "                    # Update progress\n",
        "                    progress_bar.progress((i + 1) / len(st.session_state.api_results))\n",
        "                    # Small delay to show progress bar updates smoothly\n",
        "                    time.sleep(0.05)\n",
        "\n",
        "                st.success(\"Analysis complete!\")\n",
        "\n",
        "        # Display articles in expandable sections\n",
        "        for i, article in enumerate(st.session_state.api_results):\n",
        "            with st.expander(f\"{i+1}. {article.get('title', 'Untitled')}\"):\n",
        "                st.markdown(f\"**Source:** {article.get('source', {}).get('name', 'Unknown')}\")\n",
        "                st.markdown(f\"**Published:** {article.get('publishedAt', 'Unknown')}\")\n",
        "                st.markdown(f\"**Description:** {article.get('description', 'No description')}\")\n",
        "\n",
        "                # Quick analyze button for individual article\n",
        "                if st.button(f\"Analyze Article {i+1}\"):\n",
        "                    title = article.get('title', '')\n",
        "                    description = article.get('description', '')\n",
        "                    content = article.get('content', '')\n",
        "\n",
        "                    # Combine text for analysis\n",
        "                    text = f\"{title}. {description} {content}\"\n",
        "\n",
        "                    # Analyze sentiment\n",
        "                    analysis_result = analyze_sentiment(text)\n",
        "\n",
        "                    if analysis_result:\n",
        "                        # Display sentiment badge\n",
        "                        sentiment_color = {\n",
        "                            \"Positive\": \"green\",\n",
        "                            \"Neutral\": \"blue\",\n",
        "                            \"Negative\": \"red\"\n",
        "                        }\n",
        "\n",
        "                        st.markdown(f\"\"\"\n",
        "                        <div style=\"display: flex; align-items: center; margin: 10px 0;\">\n",
        "                            <div style=\"background-color: {sentiment_color[analysis_result['sentiment']]};\n",
        "                                color: white; padding: 8px 16px; border-radius: 20px; font-weight: bold;\">\n",
        "                                {analysis_result['sentiment']}\n",
        "                            </div>\n",
        "                            <div style=\"margin-left: 15px; font-size: 16px;\">\n",
        "                                Confidence: {analysis_result['confidence']:.2f}\n",
        "                            </div>\n",
        "                        </div>\n",
        "                        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "                        # Add to history\n",
        "                        add_to_history(title, analysis_result)\n",
        "\n",
        "                # Link to the article\n",
        "                st.markdown(f\"[Read full article]({article.get('url', '#')})\")\n",
        "\n",
        "# Tabs for different visualizations of the history data\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.markdown(\"### History & Visualization\")\n",
        "show_history = st.sidebar.checkbox(\"Show Analysis History\", value=st.session_state.show_history)\n",
        "st.session_state.show_history = show_history\n",
        "\n",
        "if show_history and not st.session_state.analyzed_data.empty:\n",
        "    st.markdown(\"## Analysis History\")\n",
        "\n",
        "    # Create tabs for different visualizations\n",
        "    tab1, tab2, tab3, tab4 = st.tabs([\"Summary\", \"Trends\", \"Details\", \"Raw Data\"])\n",
        "\n",
        "    with tab1:\n",
        "        st.markdown(\"### Sentiment Distribution\")\n",
        "\n",
        "        # Create layout with columns\n",
        "        col1, col2 = st.columns(2)\n",
        "\n",
        "        with col1:\n",
        "            # Pie chart for sentiment distribution\n",
        "            sentiment_counts = st.session_state.analyzed_data['sentiment'].value_counts().reset_index()\n",
        "            sentiment_counts.columns = ['Sentiment', 'Count']\n",
        "\n",
        "            fig = px.pie(\n",
        "                sentiment_counts,\n",
        "                values='Count',\n",
        "                names='Sentiment',\n",
        "                color='Sentiment',\n",
        "                color_discrete_map={\n",
        "                    'Positive': 'green',\n",
        "                    'Neutral': 'blue',\n",
        "                    'Negative': 'red'\n",
        "                },\n",
        "                title=\"Sentiment Distribution\"\n",
        "            )\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "        with col2:\n",
        "            # Average confidence by sentiment\n",
        "            avg_confidence = st.session_state.analyzed_data.groupby('sentiment')['confidence'].mean().reset_index()\n",
        "            avg_confidence.columns = ['Sentiment', 'Avg Confidence']\n",
        "\n",
        "            fig = px.bar(\n",
        "                avg_confidence,\n",
        "                x='Sentiment',\n",
        "                y='Avg Confidence',\n",
        "                color='Sentiment',\n",
        "                color_discrete_map={\n",
        "                    'Positive': 'green',\n",
        "                    'Neutral': 'blue',\n",
        "                    'Negative': 'red'\n",
        "                },\n",
        "                title=\"Average Confidence by Sentiment\"\n",
        "            )\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "        # Summary statistics\n",
        "        st.markdown(\"### Summary Statistics\")\n",
        "\n",
        "        # Create layout with columns for stats\n",
        "        col1, col2, col3 = st.columns(3)\n",
        "\n",
        "        with col1:\n",
        "            total_items = len(st.session_state.analyzed_data)\n",
        "            st.metric(\"Total Items Analyzed\", total_items)\n",
        "\n",
        "        with col2:\n",
        "            if 'compound' in st.session_state.analyzed_data.columns:\n",
        "                avg_compound = st.session_state.analyzed_data['compound'].mean()\n",
        "                st.metric(\"Average Compound Score\", f\"{avg_compound:.2f}\")\n",
        "\n",
        "        with col3:\n",
        "            if 'confidence' in st.session_state.analyzed_data.columns:\n",
        "                avg_confidence = st.session_state.analyzed_data['confidence'].mean()\n",
        "                st.metric(\"Average Confidence\", f\"{avg_confidence:.2f}\")\n",
        "\n",
        "    with tab2:\n",
        "        st.markdown(\"### Sentiment Trends Over Time\")\n",
        "\n",
        "        # Convert timestamp to datetime if it's not already\n",
        "        if 'timestamp' in st.session_state.analyzed_data.columns:\n",
        "            try:\n",
        "                st.session_state.analyzed_data['timestamp'] = pd.to_datetime(st.session_state.analyzed_data['timestamp'])\n",
        "                # Sort by timestamp\n",
        "                trend_data = st.session_state.analyzed_data.sort_values('timestamp')\n",
        "\n",
        "                # Line chart for compound score over time\n",
        "                fig = px.line(\n",
        "                    trend_data,\n",
        "                    x='timestamp',\n",
        "                    y='compound',\n",
        "                    title=\"Sentiment Compound Score Over Time\",\n",
        "                    labels={'compound': 'Compound Score', 'timestamp': 'Time'}\n",
        "                )\n",
        "\n",
        "                # Add horizontal lines for reference\n",
        "                fig.add_shape(\n",
        "                    type=\"line\",\n",
        "                    x0=trend_data['timestamp'].min(),\n",
        "                    y0=0.05,\n",
        "                    x1=trend_data['timestamp'].max(),\n",
        "                    y1=0.05,\n",
        "                    line=dict(color=\"green\", width=1, dash=\"dash\"),\n",
        "                )\n",
        "\n",
        "                fig.add_shape(\n",
        "                    type=\"line\",\n",
        "                    x0=trend_data['timestamp'].min(),\n",
        "                    y0=-0.05,\n",
        "                    x1=trend_data['timestamp'].max(),\n",
        "                    y1=-0.05,\n",
        "                    line=dict(color=\"red\", width=1, dash=\"dash\"),\n",
        "                )\n",
        "\n",
        "                fig.add_annotation(\n",
        "                    x=trend_data['timestamp'].min(),\n",
        "                    y=0.05,\n",
        "                    text=\"Positive Threshold\",\n",
        "                    showarrow=False,\n",
        "                    yshift=10,\n",
        "                    font=dict(size=10, color=\"green\")\n",
        "                )\n",
        "\n",
        "                fig.add_annotation(\n",
        "                    x=trend_data['timestamp'].min(),\n",
        "                    y=-0.05,\n",
        "                    text=\"Negative Threshold\",\n",
        "                    showarrow=False,\n",
        "                    yshift=-10,\n",
        "                    font=dict(size=10, color=\"red\")\n",
        "                )\n",
        "\n",
        "                st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "                # Moving average of sentiment (if enough data points)\n",
        "                if len(trend_data) > 5:\n",
        "                    st.markdown(\"### Moving Average Sentiment\")\n",
        "\n",
        "                    # Create moving average\n",
        "                    trend_data['compound_ma'] = trend_data['compound'].rolling(window=min(5, len(trend_data))).mean()\n",
        "\n",
        "                    fig = px.line(\n",
        "                        trend_data.dropna(),\n",
        "                        x='timestamp',\n",
        "                        y=['compound', 'compound_ma'],\n",
        "                        title=\"Raw and Moving Average Sentiment\",\n",
        "                        labels={\n",
        "                            'compound': 'Compound Score',\n",
        "                            'compound_ma': 'Moving Avg (5)',\n",
        "                            'timestamp': 'Time'\n",
        "                        }\n",
        "                    )\n",
        "                    st.plotly_chart(fig, use_container_width=True)\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error processing time data: {str(e)}\")\n",
        "\n",
        "    with tab3:\n",
        "        st.markdown(\"### Sentiment Details\")\n",
        "\n",
        "        # Distribution of scores\n",
        "        st.markdown(\"#### Score Distributions\")\n",
        "\n",
        "        # Create a melted dataframe for the scores\n",
        "        score_cols = ['pos_score', 'neu_score', 'neg_score']\n",
        "        if all(col in st.session_state.analyzed_data.columns for col in score_cols):\n",
        "            score_data = st.session_state.analyzed_data[score_cols].melt()\n",
        "            score_data.columns = ['Score Type', 'Value']\n",
        "\n",
        "            # Map to nicer names\n",
        "            score_data['Score Type'] = score_data['Score Type'].map({\n",
        "                'pos_score': 'Positive',\n",
        "                'neu_score': 'Neutral',\n",
        "                'neg_score': 'Negative'\n",
        "            })\n",
        "\n",
        "            # Histogram of scores\n",
        "            fig = px.histogram(\n",
        "                score_data,\n",
        "                x='Value',\n",
        "                color='Score Type',\n",
        "                barmode='overlay',\n",
        "                opacity=0.7,\n",
        "                color_discrete_map={\n",
        "                    'Positive': 'green',\n",
        "                    'Neutral': 'blue',\n",
        "                    'Negative': 'red'\n",
        "                },\n",
        "                title=\"Distribution of Sentiment Scores\"\n",
        "            )\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "        # Scatter plot of compound vs confidence\n",
        "        if 'compound' in st.session_state.analyzed_data.columns and 'confidence' in st.session_state.analyzed_data.columns:\n",
        "            st.markdown(\"#### Compound Score vs. Confidence\")\n",
        "\n",
        "            fig = px.scatter(\n",
        "                st.session_state.analyzed_data,\n",
        "                x='compound',\n",
        "                y='confidence',\n",
        "                color='sentiment',\n",
        "                color_discrete_map={\n",
        "                    'Positive': 'green',\n",
        "                    'Neutral': 'blue',\n",
        "                    'Negative': 'red'\n",
        "                },\n",
        "                hover_data=['text'],\n",
        "                title=\"Compound Score vs. Confidence\"\n",
        "            )\n",
        "\n",
        "            # Add vertical reference lines\n",
        "            fig.add_shape(\n",
        "                type=\"line\",\n",
        "                x0=0.05,\n",
        "                y0=0,\n",
        "                x1=0.05,\n",
        "                y1=1,\n",
        "                line=dict(color=\"green\", width=1, dash=\"dash\"),\n",
        "            )\n",
        "\n",
        "            fig.add_shape(\n",
        "                type=\"line\",\n",
        "                x0=-0.05,\n",
        "                y0=0,\n",
        "                x1=-0.05,\n",
        "                y1=1,\n",
        "                line=dict(color=\"red\", width=1, dash=\"dash\"),\n",
        "            )\n",
        "\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "    with tab4:\n",
        "        st.markdown(\"### Raw Data\")\n",
        "\n",
        "        # Display raw data table\n",
        "        st.dataframe(st.session_state.analyzed_data)\n",
        "\n",
        "        # Add download button for CSV\n",
        "        csv = st.session_state.analyzed_data.to_csv(index=False)\n",
        "        st.download_button(\n",
        "            label=\"Download Data as CSV\",\n",
        "            data=csv,\n",
        "            file_name=\"sentiment_analysis_data.csv\",\n",
        "            mime=\"text/csv\",\n",
        "        )\n",
        "\n",
        "# About section\n",
        "st.sidebar.markdown(\"---\")\n",
        "with st.sidebar.expander(\"About This Dashboard\"):\n",
        "    st.markdown(\"\"\"\n",
        "    ### Sentiment Analysis Dashboard\n",
        "\n",
        "    This dashboard analyzes sentiment in text using VADER Sentiment Analysis with custom logic.\n",
        "\n",
        "    #### Features:\n",
        "    - Manual text analysis\n",
        "    - News API integration\n",
        "    - Custom confidence scoring\n",
        "    - Visualization of sentiment data\n",
        "    - Historical trend analysis\n",
        "\n",
        "    #### How It Works:\n",
        "    1. Enter text or fetch from News API\n",
        "    2. VADER Sentiment Analysis is applied\n",
        "    3. Custom logic enhances the analysis\n",
        "    4. Results are visualized in the dashboard\n",
        "\n",
        "    #### Sentiment Categories:\n",
        "    - Positive: Compound score >= 0.05\n",
        "    - Neutral: Compound score between -0.05 and 0.05\n",
        "    - Negative: Compound score <= -0.05\n",
        "    \"\"\")\n",
        "\n",
        "# Clear history button\n",
        "if not st.session_state.analyzed_data.empty:\n",
        "    if st.sidebar.button(\"Clear Analysis History\"):\n",
        "        st.session_state.analyzed_data = pd.DataFrame(columns=['text', 'pos_score', 'neu_score', 'neg_score', 'compound', 'sentiment', 'confidence', 'timestamp'])\n",
        "        st.sidebar.success(\"History cleared!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1gmZPFBlHG1",
        "outputId": "ddd22191-1c08-4f73-9ec5-9dc9fc5a4873"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sentiment_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "ngrok.kill()\n",
        "os.system('pkill ngrok')\n",
        "ngrok.set_auth_token(\"2vbuJtKmjEPjgnqYitA5hA8QKmR_776hVR3ruaifkgxMapJDg\")\n",
        "public_url = ngrok.connect(addr='8501')\n",
        "print(f\"ðŸš€ Fresh tunnel: {public_url}\")\n",
        "!streamlit run sentiment.py --server.port 8501 &>/dev/null &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdSeFkBtlZa0",
        "outputId": "4519da96-fea3-4afb-e2bf-18485101f7d7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Fresh tunnel: NgrokTunnel: \"https://b179-34-86-25-252.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rE3MgxFFllyH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhX/L5uzgwEdUcLjS3Grzi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}